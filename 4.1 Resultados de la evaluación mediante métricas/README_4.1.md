
# ğŸ“Š VisualizaciÃ³n de Resultados: EvaluaciÃ³n de Modelos LLM

Este mÃ³dulo forma parte del apartado **4.1 Resultados de la evaluaciÃ³n mediante mÃ©tricas** del TFG *"ImplementaciÃ³n de un sistema de evaluaciÃ³n de capacidades de grandes modelos de lenguaje a travÃ©s de bucles de resumen y extensiÃ³n de textos"*. El objetivo principal es analizar la evoluciÃ³n de mÃ©tricas (ROUGE y BERTScore) a lo largo de iteraciones de resumen/expansiÃ³n, comparando diferentes modelos LLM como GPT-4 Mini y Gemini.

## ğŸ“ Archivos incluidos

- `graficas_comparativas_llm.py`: Script principal para:
  - Leer mÃ©tricas desde archivos Excel.
  - Transformar los datos para visualizar por iteraciÃ³n y mÃ©trica.
  - Generar grÃ¡ficos de lÃ­neas con barras de error (precision, recall, F1).
  - Comparar modelos mediante grÃ¡ficos de barras agrupadas.

## ğŸ“‚ Datos de entrada

Los datos deben estar previamente generados en formato Excel a partir de los experimentos con los modelos. En este caso, se incluyen:

ğŸ”— [https://doi.org/10.5281/zenodo.15714532](https://doi.org/10.5281/zenodo.15714532)

Archivos utilizados:
- `textos_gpt4_stats.xlsx` â€“ mÃ©tricas de evaluaciÃ³n del modelo GPT-4 Mini  
- `textos_gemini_stats.xlsx` â€“ mÃ©tricas de evaluaciÃ³n del modelo Gemini 1.5 Flash


## ğŸ“ˆ Salida

Las grÃ¡ficas generadas se guardan automÃ¡ticamente en la carpeta [`graficas_finales/`](https://drive.google.com/drive/folders/1zzASjMhB4kRCQNj8nDZHpPyGEH7oQcQu) e incluyen:

- EvoluciÃ³n de precision, recall y F1 a lo largo de iteraciones.
- Comparativa entre modelos para iteraciones clave (1, 5, 10).

## â–¶ï¸ EjecuciÃ³n

```bash
python graficas_comparativas_llm.py
```

## ğŸ“‹ Requisitos

- Python 3.8+
- pandas
- matplotlib
- seaborn
- openpyxl

Puedes instalar los requisitos con:

```bash
pip install -r requirements.txt
```

## ğŸ“„ Licencia

MIT License
