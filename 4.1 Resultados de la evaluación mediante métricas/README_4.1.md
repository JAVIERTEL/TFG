
#  Visualizaci贸n de Resultados: Evaluaci贸n de Modelos LLM

Este m贸dulo forma parte del apartado **4.1 Resultados de la evaluaci贸n mediante m茅tricas** del TFG *"Implementaci贸n de un sistema de evaluaci贸n de capacidades de grandes modelos de lenguaje a trav茅s de bucles de resumen y extensi贸n de textos"*. El objetivo principal es analizar la evoluci贸n de m茅tricas (ROUGE y BERTScore) a lo largo de iteraciones de resumen/expansi贸n, comparando diferentes modelos LLM como GPT-4 Mini y Gemini.

##  Archivos incluidos

- `graficas_comparativas_llm.py`: Script principal para:
  - Leer m茅tricas desde archivos Excel.
  - Transformar los datos para visualizar por iteraci贸n y m茅trica.
  - Generar gr谩ficos de l铆neas con barras de error (precision, recall, F1).
  - Comparar modelos mediante gr谩ficos de barras agrupadas.

##  Datos de entrada

Los datos deben estar previamente generados en formato Excel a partir de los experimentos con los modelos. En este caso, se incluyen:

- [`textos_gpt4_stats.xlsx`](https://drive.google.com/drive/folders/1zzASjMhB4kRCQNj8nDZHpPyGEH7oQcQu): M茅tricas de evaluaci贸n del modelo GPT-4 Mini.
- [`textos_gemini_stats.xlsx`](https://drive.google.com/drive/folders/1zzASjMhB4kRCQNj8nDZHpPyGEH7oQcQu): M茅tricas de evaluaci贸n del modelo Gemini 1.5 Flash.

##  Salida

Las gr谩ficas generadas se guardan autom谩ticamente en la carpeta [`graficas_finales/`](https://drive.google.com/drive/folders/1zzASjMhB4kRCQNj8nDZHpPyGEH7oQcQu) e incluyen:

- Evoluci贸n de precision, recall y F1 a lo largo de iteraciones.
- Comparativa entre modelos para iteraciones clave (1, 5, 10).

## 讹 Ejecuci贸n

```bash
python graficas_comparativas_llm.py
```

##  Requisitos

- Python 3.8+
- pandas
- matplotlib
- seaborn
- openpyxl

Puedes instalar los requisitos con:

```bash
pip install -r requirements.txt
```

##  Licencia

MIT License
