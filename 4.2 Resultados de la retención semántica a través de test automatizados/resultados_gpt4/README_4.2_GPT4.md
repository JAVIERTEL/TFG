# ğŸ“Š EvaluaciÃ³n de respuestas generadas - GPT-4

Este directorio forma parte del sistema desarrollado para evaluar la retenciÃ³n de informaciÃ³n semÃ¡ntica por parte de modelos LLM, comparando respuestas generadas por GPT-4 frente a una baterÃ­a de preguntas tipo test generadas a partir de los textos originales.

## ğŸ§© Estructura de scripts

- `respuestas_to_excel_gpt4.py`: Extrae las respuestas generadas por GPT-4 desde un archivo `.jsonl` (`benchmark_batch_chat_gpt4_v2-answer.jsonl`) y las transforma en una tabla Excel estructurada, con columnas para `id`, `pregunta`, `tipo_texto` y `respuesta`.
- `resultados_gpt-4.py`: Compara las respuestas generadas por GPT-4 con las respuestas correctas de referencia, determinando si son correctas, incorrectas o invÃ¡lidas. El resultado se guarda en `resultados_gpt4.xlsx`.
- `representacion_resultados_gpt-4.py`: Genera visualizaciones grÃ¡ficas de los resultados de evaluaciÃ³n, mostrando el rendimiento de GPT-4 en distintas versiones del texto (original, resumido y expandido) por iteraciones.

## ğŸ—‚ï¸ Archivos necesarios (no incluidos en este repositorio)

Los ficheros de entrada y salida se encuentran en el siguiente enlace de Zenodo:

ğŸ”— [Excel y GrÃ¡fica â€“ Resultados GPT-4](https://doi.org/10.5281/zenodo.15714532)


## âœï¸ Autor

Javier GonzÃ¡lez PÃ©rez Â· Trabajo Fin de Grado - UPM

---

### Licencia

Este proyecto estÃ¡ licenciado bajo la licencia MIT. Puedes consultar los tÃ©rminos completos [aquÃ­](https://opensource.org/licenses/MIT).
